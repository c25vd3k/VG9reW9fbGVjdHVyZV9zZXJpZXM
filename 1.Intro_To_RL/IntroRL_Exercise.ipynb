{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIwkMy9WnrRn"
   },
   "source": [
    "# <center> Introduction to Reinforcement Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVU9RrU9rRB"
   },
   "source": [
    "# Getting Used to the Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY8sb3WLnrT6"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHhQP_yrnrUA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from IntroRL_Support.helper import *\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from ece4078.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "UP = 0; DOWN = 1; LEFT = 2; RIGHT = 3; STAY = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get used to some function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__get_reachable_states__(s)` given a state `s`, what is the state that the agent might end up in for all possible actions, remember that the grid has borders, and the agent cannot go into obstacle tile.\n",
    "\n",
    "Try to reason with yourself the output of the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "grid_world.__get_reachable_states__(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new transition function\n",
    "\n",
    "Let's make the environment such that there is an $p$ chance that the agent will move as we expected, and $1 - p$ chance that it will stay exactly where it is. Given $|p| \\leq 1$.\n",
    "\n",
    "Note that obstacle is a state with value `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_LUT(env, p):\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    state_transitions = np.zeros((state_size, action_size, state_size))\n",
    "\n",
    "    #TODO 1: Compute new transition function -----------------------------------------\n",
    "    \n",
    "    #ENDTODO -------------------------------------------------------------------------\n",
    "    return state_transitions\n",
    "\n",
    "state_transitions = generate_transition_LUT(grid_world, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new reward function\n",
    "\n",
    "Let's make the reward such that moving UP is considered a really bad thing. Make the reward such that whenever you move up or down, the reward is subtracted by `up_down_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_action_reward_LUT(env, up_down_penalty):\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    rewards = np.zeros((state_size, action_size))\n",
    "\n",
    "    #TODO 2: Compute new reward function --------------------------------------------\n",
    "    \n",
    "    #ENDTODO -------------------------------------------------------------------------\n",
    "    return rewards\n",
    "\n",
    "rewards = generate_state_action_reward_LUT(grid_world, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new state-dependent policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(up_chance, down_chance, left_chance, right_chance):\n",
    "    #TODO 3: Define stochastic policy ------------------------------------------------\n",
    "    pass\n",
    "    #ENDTODO -------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IntroRL_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
